{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some initial thoughts on \"simple fish\".  I.e., iterative maps that the following properties.\n",
    "\n",
    "1. They are as simple as possible (which is, of course, in the eye of the beholder).\n",
    "2. They include the 0 function.\n",
    "3. They include the identity function.\n",
    "4. They can be linear/affine or non-linear.\n",
    "5. They have a \"waist\" (i.e., they can be restricted to a low dimensional manifold of a prescribed dimension).\n",
    "6. They can be initialized using PCA. \n",
    "\n",
    "That seems like a lot to ask!  I mean, MLPs do *not* have many of those properties.   Fortuantely, I think there is a path forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear\n",
    "\n",
    "$Wx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine\n",
    "\n",
    "$W \\cdot x + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squared nonlinearity\n",
    "\n",
    "It is tempting (and many give in to the temptation) to write\n",
    "\n",
    "$(W \\cdot x)^2$ \n",
    "\n",
    "Of course, this makes no sense! ðŸ˜Ÿ \n",
    "\n",
    "What is really meant is that we do an entrywise squaring of the vector $(W \\cdot x + b)$, which can be written as\n",
    "\n",
    "\n",
    "$(W \\cdot x) \\odot (W \\cdot x)$\n",
    "\n",
    "where $\\odot$ is the Hadamard (or entrywise) product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again we are mislead by our scalar thinking\n",
    "\n",
    "For scalar variables it is quite natural to think of squaring as the simplest nonlinearity, but in the vector case we are being a bit provincial if we do that.  E.g. why is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\odot\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_1^2 \\\\\n",
    "x_2^2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "any simpler than\n",
    "$$\n",
    "x_1\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_1 x_1 \\\\\n",
    "x_1 x_2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In fact, the later is perhaps even simpler in that it is a (non-linear) scalar product rather than a scary Hadamard product ðŸ˜Ž."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The simplest non-linear map that makes Randy happy\n",
    "\n",
    "So, all that ink spilled, what is the simplest non-linear map that makes Randy happy?  Well, it is\n",
    "\n",
    "$$\n",
    "(W_1 \\cdot x + b_1) \\odot (W_2 \\cdot x + b_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check all of our constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. They are as simple as possible (which is, of course, in the eye of the beholder).\n",
    "\n",
    "By construction ðŸ˜‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. They include the 0 function.\n",
    "\n",
    "$$\n",
    "(0 \\cdot x + 0) \\odot (0 \\cdot x + 0) = 0\n",
    "$$\n",
    "\n",
    "In fact, they include many zero functions, for example here is another\n",
    "\n",
    "$$\n",
    "(W_1 \\cdot x + b_1) \\odot (0 \\cdot x + 0) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. They include the identity function.\n",
    "\n",
    "$$\n",
    "(I \\cdot x + 0) \\odot (0 \\cdot x + 1) = x \\odot 1 = x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. They can be linear/affine or non-linear.\n",
    "\n",
    "Nonlinearity by construction ðŸ˜‹\n",
    "\n",
    "Every affine map is a special case of this map, e.g.,\n",
    "\n",
    "$$\n",
    "(W_1 \\cdot x + b_1) \\odot (0 \\cdot x + 1) = (W_1 \\cdot x + b_1) \\odot 1 = W_1 \\cdot x + b_1\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. They have a \"waist\" (i.e., they can be restricted to a low dimensional manifold of a prescribed dimension).\n",
    "\n",
    "This is the most interesting part.  Let's look at the map\n",
    "\n",
    "$$\n",
    "( (U_1 \\cdot V^T) \\cdot x + b_1) \\odot ( (U_2 \\cdot V^T) \\cdot x + b_2) = \\\\\n",
    "( U_1 \\cdot (V^T \\cdot x) + b_1) \\odot ( U_2 \\cdot (V^T \\cdot x) + b_2) = \\\\\n",
    "( U_1 \\cdot y + b_1) \\odot ( U_2 \\cdot y + b_2) = \\\\\n",
    "f(y)\n",
    "$$\n",
    "\n",
    "where $y = V^T \\cdot x$ and $f(y)$ is desired map.  Now, if we have that $U_1, U_2, V \\in \\mathbb{R}^{D \\times d}$, where $d$ is the dimension of the manifold we want to restrict to, then we have a function of $d$ parameters embedded in a $D$ dimensional space. I.e., a $d$-dimensional manifold in $D$ dimensional space. This is exactly the \"waist\" that we are looking for. \n",
    "\n",
    "Note, there are two interesting cases here.  \n",
    "\n",
    "First, we can *define* $W_1 = U_1 \\cdot V^T$ and $W_2 = U_2 \\cdot V^T$ and then do gradient descent on the parameters matrices $U_1,U_2,V$. In this case we will do gradient descent to search for a minimizer over a space of $d$-dimensional manifolds.\n",
    "\n",
    "Second, we can *initialize* $W_1 = U_1 \\cdot V^T$ and $W_2 = U_2 \\cdot V^T$ and then do gradient descent on the parameters matrices $W_1,W_2$. In this case we will start our gradient descent search on the given $d$-dimensional manifold, but the minimizer we find may be of any dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. They can be initialized using PCA.\n",
    "\n",
    "Given 5. above, this is trivial.  Given a data matrix $X \\in \\mathbb{R}^{D \\times N}$, we can compute the SVD $X = U \\Sigma V^T$ and then set $W_1 = U \\Sigma U^T$.  Then\n",
    "\n",
    "$$\n",
    "( (U \\cdot U^T) \\cdot x + 0) \\odot (0 \\cdot x + 1) = \\\\\n",
    "( (U \\cdot U^T) \\cdot x ) \\odot 1 = \\\\\n",
    "( U \\cdot U^T) \\cdot x \n",
    "$$\n",
    "\n",
    "which is precisely the PCA projection of $X$ onto the span of the columns of $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some numerical experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the transformation to apply to the MNIST images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create a list of tensors, where each tensor is an MNIST image\n",
    "image_tensors = [image for image, label in mnist_dataset]\n",
    "\n",
    "# Stack the tensors along the second dimension to create a tensor where each column is an MNIST image\n",
    "mnist_tensor = torch.stack(image_tensors, dim=1)\n",
    "\n",
    "# Print the shape of the tensor\n",
    "print(mnist_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "X = mnist_tensor.view(60000, 784).float().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(X[:, 0].view(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "U, S, V = torch.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(S)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "strangeI = U[:, :200] @ U[:, :200].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(strangeI)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "XHat = strangeI @ X"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(10, 2, figsize=(10, 20))\n",
    "for i in range(10):\n",
    "    ax[i, 0].imshow(X[:, i].view(28, 28), cmap='gray')\n",
    "    ax[i, 1].imshow(XHat[:, i].view(28, 28), cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another one that is important, I think\n",
    "\n",
    "Looking at our constraints, the is a but of ambiguity.  In particular, we have\n",
    "\n",
    "1. They are as simple as possible (which is, of course, in the eye of the beholder).\n",
    "2. They include the 0 function.\n",
    "3. They include the identity function.\n",
    "4. ***They can be linear/affine or non-linear.***\n",
    "5. They have a \"waist\" (i.e., they can be restricted to a low dimensional manifold of a prescribed dimension).\n",
    "6. They can be initialized using PCA. \n",
    "\n",
    "And there are several ways to think about 4!  Abve we use a *multiplicative* non-linearity, but there are other important non-linearities.  For example, a *discontinuous* non-linearity! ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-linearity above is far from discontinuous.  In fact, it is polynomial, which is about as smooth as you can get.  But, what if we use a discontinuous non-linearity?  \n",
    "\n",
    "For example, how about\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    W_1 \\cdot x + b_1 & \\text{if } x < 0 \\\\\n",
    "    W_2 \\cdot x + b_2 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "again, we are being provincial in our thinking since what does it mean to be less than zero for a vector?  so, we can be a little more precise and write\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    W_1 \\cdot x + b_1 & \\text{if } x_j < 0 \\\\\n",
    "    W_2 \\cdot x + b_2 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. They are as simple as possible (which is, of course, in the eye of the beholder).\n",
    "\n",
    "By construction ðŸ˜‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. They include the 0 function.\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    0 \\cdot x + 0 & \\text{if } x_j < 0 \\\\\n",
    "    0 \\cdot x + 0 & \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. They include the identity function.\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    I \\cdot x + 0 & \\text{if } x_j < 0 \\\\\n",
    "    I \\cdot x + 0 & \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. They can be linear/affine or non-linear.\n",
    "\n",
    "Nonlinearity by construction ðŸ˜‹\n",
    "\n",
    "Again, every affine map is a special case of this map, e.g.,\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    W \\cdot x + b & \\text{if } x_j < 0 \\\\\n",
    "    W \\cdot x + b & \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. They have a \"waist\" (i.e., they can be restricted to a low dimensional manifold of a prescribed dimension).\n",
    "\n",
    "As before, this can be\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    (U_1 \\cdot V^T) \\cdot x + b_1 & \\text{if } x_j < 0 \\\\\n",
    "    (U_2 \\cdot V^T) \\cdot x + b_2 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $y = V^T \\cdot x$ and $f(y)$ is desired map.  Now, if we have that $U_1, U_2, V \\in \\mathbb{R}^{D \\times d}$, where $d$ is the dimension of the manifold we want to restrict to, then we have a function of $d$ parameters embedded in a $D$ dimensional space. I.e., a $d$-dimensional manifold in $D$ dimensional space. This is exactly the \"waist\" that we are looking for. \n",
    "\n",
    "Note, there are two interesting cases here.  \n",
    "\n",
    "First, we can *define* $W_1 = U_1 \\cdot V^T$ and $W_2 = U_2 \\cdot V^T$ and then do gradient descent on the parameters matrices $U_1,U_2,V$. In this case we will do gradient descent to search for a minimizer over a space of $d$-dimensional manifolds.\n",
    "\n",
    "Second, we can *initialize* $W_1 = U_1 \\cdot V^T$ and $W_2 = U_2 \\cdot V^T$ and then do gradient descent on the parameters matrices $W_1,W_2$. In this case we will start our gradient descent search on the given $d$-dimensional manifold, but the minimizer we find may be of any dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. They can be initialized using PCA.\n",
    "\n",
    "Given 5. above, this is trivial.  Given a data matrix $X \\in \\mathbb{R}^{D \\times N}$, we can compute the SVD $X = U \\Sigma V^T$ and then set $W_1 = U \\Sigma U^T$.  Then\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    (U \\cdot U^T) \\cdot x + 0 & \\text{if } x_j < 0 \\\\\n",
    "    (U \\cdot U^T) \\cdot x + 0 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which is precisely the PCA projection of $X$ onto the span of the columns of $U$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. This is actually a version of ReLU ðŸ˜‚\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    0 \\cdot x + 0 & \\text{if } x_j < 0 \\\\\n",
    "    W_2 \\cdot x + b_2 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Combining Simple Models\n",
    "\n",
    "2 Hadamard -> 1 Piecewise:\n",
    "\n",
    "Models 1 and 2:\n",
    "$$\n",
    "( (U \\cdot U^T) \\cdot x + 0) \\odot (0 \\cdot x + 1) = \\\\\n",
    "( (U \\cdot U^T) \\cdot x ) \\odot 1 = \\\\\n",
    "( U \\cdot U^T) \\cdot x \n",
    "$$\n",
    "\n",
    "Combined Model:\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    \\text{model1}(x) & \\text{if } x_j < 0 \\\\\n",
    "    \\text{model2}(x) & \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 Piecewise -> 1 Hadamard:\n",
    "\n",
    "Model1:\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    (U \\cdot U^T) \\cdot x + 0 & \\text{if } x_j < 0 \\\\\n",
    "    (U \\cdot U^T) \\cdot x + 0 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Model2:\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "    0 \\cdot x + 1 & \\text{if } x_j < 0 \\\\\n",
    "    0 \\cdot x + 1 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Combined Model:\n",
    "$$\n",
    "\\text{model1}(x) \\odot \\text{model2}(x)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
